{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation d'un réseau de neurone convolutionnel pour la désagrégation de consommation\n",
    "\n",
    "Dans ce notebook on applique un réseau de neurone convolutionnel pour désagréger la consommation pour un appareil. La méthode est reprise en grande partie du travail de Jake Kelly (https://arxiv.org/pdf/1507.06594.pdf ,https://github.com/JackKelly/neuralnilm). On utilisera la bibliothèque `neuralnilm` pour créer le pipeline de données de training. On utilise la bibliothèque `keras` pour le réseau de neurone.\n",
    "\n",
    "On travaille sur le jeu de données REDD \"low_freq\" disponible ici : http://redd.csail.mit.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nilmtk\n",
    "from nilmtk import DataSet, MeterGroup\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "# import dataset\n",
    "\n",
    "dataset = DataSet(\"data/REDD/redd.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bâtiment 1 , 16 appareils\n",
      "Appliance(type='sockets', instance=4)\n",
      "Appliance(type='light', instance=3)\n",
      "Appliance(type='washer dryer', instance=1)\n",
      "Appliance(type='light', instance=2)\n",
      "Appliance(type='sockets', instance=3)\n",
      "Appliance(type='electric stove', instance=1)\n",
      "Appliance(type='electric oven', instance=1)\n",
      "Appliance(type='light', instance=1)\n",
      "Appliance(type='electric space heater', instance=1)\n",
      "Appliance(type='microwave', instance=1)\n",
      "Appliance(type='unknown', instance=2)\n",
      "Appliance(type='sockets', instance=2)\n",
      "Appliance(type='fridge', instance=1)\n",
      "Appliance(type='dish washer', instance=1)\n",
      "Appliance(type='sockets', instance=1)\n",
      "Appliance(type='unknown', instance=1)\n",
      "bâtiment 2 , 9 appareils\n",
      "Appliance(type='washer dryer', instance=1)\n",
      "Appliance(type='waste disposal unit', instance=1)\n",
      "Appliance(type='electric stove', instance=1)\n",
      "Appliance(type='light', instance=1)\n",
      "Appliance(type='microwave', instance=1)\n",
      "Appliance(type='sockets', instance=2)\n",
      "Appliance(type='fridge', instance=1)\n",
      "Appliance(type='dish washer', instance=1)\n",
      "Appliance(type='sockets', instance=1)\n",
      "bâtiment 3 , 19 appareils\n",
      "Appliance(type='sockets', instance=3)\n",
      "Appliance(type='CE appliance', instance=1)\n",
      "Appliance(type='unknown', instance=1)\n",
      "Appliance(type='smoke alarm', instance=1)\n",
      "Appliance(type='light', instance=5)\n",
      "Appliance(type='washer dryer', instance=1)\n",
      "Appliance(type='light', instance=2)\n",
      "Appliance(type='sockets', instance=5)\n",
      "Appliance(type='sockets', instance=2)\n",
      "Appliance(type='waste disposal unit', instance=1)\n",
      "Appliance(type='light', instance=1)\n",
      "Appliance(type='light', instance=4)\n",
      "Appliance(type='fridge', instance=1)\n",
      "Appliance(type='dish washer', instance=1)\n",
      "Appliance(type='sockets', instance=1)\n",
      "Appliance(type='electric furnace', instance=1)\n",
      "Appliance(type='sockets', instance=4)\n",
      "Appliance(type='microwave', instance=1)\n",
      "Appliance(type='light', instance=3)\n",
      "bâtiment 4 , 17 appareils\n",
      "Appliance(type='unknown', instance=1)\n",
      "Appliance(type='light', instance=3)\n",
      "Appliance(type='smoke alarm', instance=1)\n",
      "Appliance(type='washer dryer', instance=1)\n",
      "Appliance(type='light', instance=2)\n",
      "Appliance(type='sockets', instance=3)\n",
      "Appliance(type='unknown', instance=3)\n",
      "Appliance(type='electric stove', instance=1)\n",
      "Appliance(type='light', instance=1)\n",
      "Appliance(type='unknown', instance=2)\n",
      "Appliance(type='light', instance=4)\n",
      "Appliance(type='air conditioner', instance=2)\n",
      "Appliance(type='sockets', instance=2)\n",
      "Appliance(type='dish washer', instance=1)\n",
      "Appliance(type='air conditioner', instance=1)\n",
      "Appliance(type='sockets', instance=1)\n",
      "Appliance(type='electric furnace', instance=1)\n",
      "bâtiment 5 , 22 appareils\n",
      "Appliance(type='subpanel', instance=2)\n",
      "Appliance(type='sockets', instance=3)\n",
      "Appliance(type='CE appliance', instance=1)\n",
      "Appliance(type='sockets', instance=6)\n",
      "Appliance(type='unknown', instance=1)\n",
      "Appliance(type='light', instance=5)\n",
      "Appliance(type='washer dryer', instance=1)\n",
      "Appliance(type='light', instance=2)\n",
      "Appliance(type='subpanel', instance=1)\n",
      "Appliance(type='sockets', instance=5)\n",
      "Appliance(type='sockets', instance=2)\n",
      "Appliance(type='waste disposal unit', instance=1)\n",
      "Appliance(type='light', instance=1)\n",
      "Appliance(type='electric space heater', instance=1)\n",
      "Appliance(type='light', instance=4)\n",
      "Appliance(type='fridge', instance=1)\n",
      "Appliance(type='dish washer', instance=1)\n",
      "Appliance(type='sockets', instance=1)\n",
      "Appliance(type='electric furnace', instance=1)\n",
      "Appliance(type='sockets', instance=4)\n",
      "Appliance(type='microwave', instance=1)\n",
      "Appliance(type='light', instance=3)\n",
      "bâtiment 6 , 14 appareils\n",
      "Appliance(type='sockets', instance=4)\n",
      "Appliance(type='washer dryer', instance=1)\n",
      "Appliance(type='sockets', instance=3)\n",
      "Appliance(type='CE appliance', instance=1)\n",
      "Appliance(type='electric stove', instance=1)\n",
      "Appliance(type='light', instance=1)\n",
      "Appliance(type='electric space heater', instance=1)\n",
      "Appliance(type='air handling unit', instance=1)\n",
      "Appliance(type='fridge', instance=1)\n",
      "Appliance(type='sockets', instance=2)\n",
      "Appliance(type='dish washer', instance=1)\n",
      "Appliance(type='air conditioner', instance=1)\n",
      "Appliance(type='sockets', instance=1)\n",
      "Appliance(type='unknown', instance=1)\n"
     ]
    }
   ],
   "source": [
    "# On peut observer le type d'appareil par maison\n",
    "for building in dataset.buildings.items():\n",
    "    print(\"bâtiment\",building[0],\",\", len(building[1].elec.appliances),\"appareils\")\n",
    "    for appliance in building[1].elec.appliances:\n",
    "        print(appliance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing des données, création des données d'entrainement et de test\n",
    "\n",
    "On va maintenant préparer les données en batch de séquences à l'aide de la bibliothèque neuralnilm. Les étapes du preprocessing sont détaillées dans le rapport.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralnilm.data'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-a9751be4c43c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnilmtk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mprint_dict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnilmtk\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataSet\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mneuralnilm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloadactivations\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mload_nilmtk_activations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mneuralnilm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msyntheticaggregatesource\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSyntheticAggregateSource\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mneuralnilm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrealaggregatesource\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRealAggregateSource\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'neuralnilm.data'"
     ]
    }
   ],
   "source": [
    "import nilmtk\n",
    "from nilmtk.utils import print_dict\n",
    "from nilmtk import DataSet\n",
    "from neuralnilm.data.loadactivations import load_nilmtk_activations\n",
    "from neuralnilm.data.syntheticaggregatesource import SyntheticAggregateSource\n",
    "from neuralnilm.data.realaggregatesource import RealAggregateSource\n",
    "from neuralnilm.data.stridesource import StrideSource\n",
    "from neuralnilm.data.datapipeline import DataPipeline\n",
    "from neuralnilm.data.processing import DivideBy, IndependentlyCenter\n",
    "\n",
    "def select_windows(train_buildings, unseen_buildings):\n",
    "    windows = {fold: {} for fold in DATA_FOLD_NAMES}\n",
    "\n",
    "    def copy_window(fold, i):\n",
    "        windows[fold][i] = WINDOWS[fold][i]\n",
    "\n",
    "    for i in train_buildings:\n",
    "        copy_window('train', i)\n",
    "        copy_window('unseen_activations_of_seen_appliances', i)\n",
    "    for i in unseen_buildings:\n",
    "        copy_window('unseen_appliances', i)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def filter_activations(windows, activations):\n",
    "    new_activations = {\n",
    "        fold: {appliance: {} for appliance in APPLIANCES}\n",
    "        for fold in DATA_FOLD_NAMES}\n",
    "    for fold, appliances in activations.items():\n",
    "        for appliance, buildings in appliances.items():\n",
    "            required_building_ids = windows[fold].keys()\n",
    "            required_building_names = [\n",
    "                'building_{}'.format(i) for i in required_building_ids]\n",
    "            for building_name in required_building_names:\n",
    "                try:\n",
    "                    new_activations[fold][appliance][building_name] = (\n",
    "                        activations[fold][appliance][building_name])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NILMTK_FILENAME = './data/REDD/redd.h5'\n",
    "\n",
    "# Période d'échantillonage la plus longue pour pouvoir comparer les données qui ont des périodes d'échantillonage différentes.\n",
    "SAMPLE_PERIOD = 3\n",
    "# Tous les appareil à considérer. Un de ces appareil sera l'appareil visé par l'algo, les autres servent à la création de données artificielle.\n",
    "APPLIANCES = ['washer dryer',\n",
    " 'fridge',\n",
    " 'microwave',\n",
    " 'dish washer']\n",
    "\n",
    "WINDOWS = {\n",
    "    'train': {\n",
    "        1: (\"2011-04-18\", \"2011-05-24\"),\n",
    "        2: (\"2011-04-17\", \"2011-05-22\"),\n",
    "        3: (\"2011-04-16\", \"2013-05-30\"),\n",
    "        6: (\"2011-05-21\", \"2011-06-14\"),\n",
    "    },\n",
    "    'unseen_activations_of_seen_appliances': {\n",
    "        1: (\"2011-04-19\", None),\n",
    "        2: (\"2011-04-19\", None),\n",
    "        3: (\"2011-04-19\", None),\n",
    "        6: (\"2011-05-22\", None),\n",
    "    },\n",
    "    'unseen_appliances': {\n",
    "        5: (\"2011-04-18\", None)\n",
    "    }\n",
    "}\n",
    "\n",
    "# get the dictionary of activations for each appliance\n",
    "activations = load_nilmtk_activations(\n",
    "    appliances=APPLIANCES,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    sample_period=SAMPLE_PERIOD,\n",
    "    windows=WINDOWS\n",
    ")\n",
    "\n",
    "# ------------\n",
    "# get pipeline for the fridge \n",
    "# ------------\n",
    "num_seq_per_batch = 16\n",
    "target_appliance = 'fridge'\n",
    "# à choisir en fonction de la longueur des activations et de la période d'échantillonage\n",
    "seq_length = 512\n",
    "train_buildings = [1, 2, 3, 6]\n",
    "unseen_buildings = [5]\n",
    "DATA_FOLD_NAMES = (\n",
    "    'train', 'unseen_appliances', 'unseen_activations_of_seen_appliances')\n",
    "\n",
    "filtered_windows = select_windows(train_buildings, unseen_buildings)\n",
    "filtered_activations = filter_activations(filtered_windows, activations)\n",
    "\n",
    "synthetic_agg_source = SyntheticAggregateSource(\n",
    "    activations=filtered_activations,\n",
    "    target_appliance=target_appliance,\n",
    "    seq_length=seq_length,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")\n",
    "\n",
    "real_agg_source = RealAggregateSource(\n",
    "    activations=filtered_activations,\n",
    "    target_appliance=target_appliance,\n",
    "    seq_length=seq_length,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=filtered_windows,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")\n",
    "# ------------\n",
    "# Normalisation des données\n",
    "# rescaling is done using the a first batch of num_seq_per_batch sequences\n",
    "sample = next(real_agg_source.get_batch(num_seq_per_batch=1024))\n",
    "sample = sample.before_processing\n",
    "input_std = sample.input.flatten().std()\n",
    "target_std = sample.target.flatten().std()\n",
    "# ------------\n",
    "\n",
    "pipeline = DataPipeline(\n",
    "    [synthetic_agg_source, real_agg_source],\n",
    "    num_seq_per_batch=num_seq_per_batch,\n",
    "    input_processing=[DivideBy(input_std), IndependentlyCenter()],\n",
    "    target_processing=[DivideBy(target_std)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-6eae6ac69e61>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mnum_test_seq\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m101\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mX_valid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_test_seq\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mnum_seq_per_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mseq_length\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[0mY_valid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_test_seq\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mnum_seq_per_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# 3 for the 3 output neurons\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# create the validation set\n",
    "# ------------\n",
    "\n",
    "num_test_seq = 101\n",
    "\n",
    "X_valid = np.empty((num_test_seq*num_seq_per_batch, seq_length))\n",
    "Y_valid = np.empty((num_test_seq*num_seq_per_batch, 3)) # 3 for the 3 output neurons\n",
    "\n",
    "for i in range(num_test_seq):\n",
    "    (x_valid,y_valid) = next(pipeline.train_generator(fold = 'unseen_appliances', source_id = 1)) #source id : 1 pour les données réelles seulement, 0 pour les données artificielles seulement.\n",
    "    X_valid[i*num_seq_per_batch: (i+1)*num_seq_per_batch,:] = x_valid[:,:,0]\n",
    "    Y_valid[i*num_seq_per_batch:  (i+1)*num_seq_per_batch,:] = y_valid\n",
    "X_valid = np.reshape(X_valid, [X_valid.shape[0],X_valid.shape[1],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Set from unseen_appliances\n",
    "\n",
    "mains_meter = dataset.buildings[unseen_buildings[0]].elec.mains()\n",
    "good_sections = mains_meter.good_sections()\n",
    "mains_data = mains_meter.power_series_all_data(sample_period=SAMPLE_PERIOD,\n",
    "                                               sections=good_sections).dropna()\n",
    "# find the number of testing sequences in the testing set\n",
    "num_test_seq = int(mains_data.shape[0] / seq_length)\n",
    "\n",
    "Y_test = np.empty((num_seq_per_batch*num_test_seq,3))\n",
    "X_test = np.empty((num_seq_per_batch*num_test_seq,seq_length))\n",
    "for i in range(num_test_seq):\n",
    "    (x_test,y_test) = next(pipeline.train_generator(fold = 'unseen_appliances', source_id = 1))\n",
    "    X_test[num_seq_per_batch*i: num_seq_per_batch*(i+1),:] = x_test[:,:,0]\n",
    "    Y_test[num_seq_per_batch*i: num_seq_per_batch*(i+1),:] = y_test\n",
    "X_test = np.reshape(X_test, [X_test.shape[0],X_test.shape[1],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network architecture\n",
    "\n",
    "On implémente le réseau de neurone avec `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, MaxPooling1D, AveragePooling1D, Conv1D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "import keras.callbacks\n",
    "import time\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "\n",
    "# chemins pour la sauvegarde des résultats\n",
    "# ------------\n",
    "exp_number = 32\n",
    "output_architecture = './tmpdata/convnet_architecture_exp' + str(exp_number) + '.json'\n",
    "best_weights_during_run = './tmpdata/weights_exp' + str(exp_number) + '.h5'\n",
    "final_weights = './tmpdata/weights_exp' + str(exp_number) + '_final.h5'\n",
    "loss_history = './tmpdata/history_exp' + str(exp_number) + '.pickle'\n",
    "# ------------\n",
    "\n",
    "# a class used to record the training and validation loss\n",
    "# at the end of each epoch\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.valid_losses.append(logs.get('val_loss'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-89a5b73a56b7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Definition du modèle\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0minput_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mseq_length\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m model = Sequential(\n\u001B[0;32m      5\u001B[0m     [\n",
      "\u001B[1;31mNameError\u001B[0m: name 'seq_length' is not defined"
     ]
    }
   ],
   "source": [
    "# Definition du modèle\n",
    "\n",
    "input_shape = (seq_length, 1)\n",
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=16,kernel_size=3,input_shape=input_shape, activation=\"relu\"),\n",
    "        Conv1D(filters=16,kernel_size=3,input_shape=input_shape, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(4096,activation=\"relu\"),\n",
    "        Dense(3072,activation=\"relu\"),\n",
    "        Dense(2048,activation=\"relu\"),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(3, activation=\"linear\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the loss history\n",
    "history = LossHistory()\n",
    "# save the weigths when the vlaidation lost decreases only\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath=best_weights_during_run, save_best_only=True, verbose =1 )\n",
    "#tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(pipeline.train_generator(fold = 'train'),\n",
    "          epochs=20,\n",
    "          validation_data=(x_valid,y_valid),\n",
    "          max_queue_size= 50,\n",
    "          steps_per_epoch=8000/num_seq_per_batch,\n",
    "          callbacks=[history, checkpointer, tensorboard_callback],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des résultats\n",
    "\n",
    "losses_dic = {'train_loss': history.train_losses, 'valid_loss':history.valid_losses}\n",
    "# save history\n",
    "losses_dic = {'train_loss': history.train_losses, 'valid_loss':history.valid_losses}\n",
    "with open(loss_history, 'wb') as handle:\n",
    "  pickle.dump(losses_dic, handle)\n",
    "\n",
    "print('\\n saving the architecture of the model \\n')\n",
    "json_string = model.to_json()\n",
    "open(output_architecture, 'w').write(json_string)\n",
    "\n",
    "print('\\n saving the final weights ... \\n')\n",
    "model.save_weights(final_weights, overwrite = True)\n",
    "print('done saving the weights')\n",
    "\n",
    "print('\\n saving the training and validation losses')\n",
    "\n",
    "print('This was the model trained')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import model_from_json\n",
    "# load experiment number\n",
    "exp_number = 31\n",
    "#load model architecture\n",
    "model_architecture = './tmpdata/convnet_architecture_exp' + str(exp_number) + '.json'\n",
    "# load the weights for the lowest validation loss during training\n",
    "best_weights_during_run = './tmpdata/weights_exp' + str(exp_number) + '.h5'\n",
    "# load the final weights at the end of the 20 epochs\n",
    "final_weights = './tmpdata/weights_exp' + str(exp_number) + '_final.h5'\n",
    "# load model form json\n",
    "model = model_from_json(open(model_architecture).read())\n",
    "\n",
    "# load intermediate or final weights\n",
    "model.load_weights(best_weights_during_run)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n",
    "# print the summary of the architecture\n",
    "model.summary()\n",
    "# load the loss summary (training and validation losses)\n",
    "import pickle\n",
    "losses = pickle.load( open('./tmpdata/history_exp' + str(exp_number) + '.pickle'   , 'rb'))\n",
    "# load the test set\n",
    "test_set = pickle.load( open('neuralnilm/tmpdata/TestSetRePickled.pickle', 'rb'))\n",
    "X_test = test_set['X_test']\n",
    "Y_test = test_set[\"Y_test\"]\n",
    "\n",
    "# Here we predict the output from the neural network and show the scores\n",
    "from neuralnilm.scores import scores\n",
    "Y_pred = model.predict(X_test)\n",
    "scores(Y_pred, Y_test, print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut afficher quelques résultats visuellement pour se faire une idée \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nilm2",
   "language": "python",
   "display_name": "NILM2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}