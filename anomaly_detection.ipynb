{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moving-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector, Flatten\n",
    "from keras.models import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "north-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset to import\n",
    "\n",
    "ignore_ids = [223, 45, 19, 105, 75, 63, 58, 59]\n",
    "path_data = 'C:/Users/Orson/Documents/Digital_Lab/Projet_GAC/Datasets/'\n",
    "\n",
    "def get_selected_ids(selection):\n",
    "    # Get ids\n",
    "    meta = pd.read_csv(path_data+\"metadata.csv\", sep=';')\n",
    "    # Use selection\n",
    "    for select in selection:\n",
    "        if select.startswith(\"max\"):\n",
    "            if selection[select] is not None:\n",
    "                col = select[4:]\n",
    "                meta = meta[meta[col] <= selection[select]]\n",
    "        elif select.startswith(\"min\"):\n",
    "            if selection[select] is not None:\n",
    "                col = select[4:]\n",
    "                meta = meta[meta[col] >= selection[select]]\n",
    "        elif select.startswith(\"in\"):\n",
    "            if selection[select]:\n",
    "                col = select[3:]\n",
    "                meta = meta[meta[col].isin(selection[select])]\n",
    "        else:\n",
    "            col = select\n",
    "            meta = meta[meta[col] == selection[col]]\n",
    "    # Remove manual ids\n",
    "    meta = meta[~meta[\"bat_id\"].isin(ignore_ids)]\n",
    "    ids = meta[\"bat_id\"].values\n",
    "    return ids\n",
    "\n",
    "# get list of buildings data corresponding to the ids selected\n",
    "\n",
    "def get_list_of_datasets(ids):\n",
    "    list_of_datasets = []\n",
    "    list_of_ids = []\n",
    "    for filename in os.listdir(path_data):\n",
    "        if re.fullmatch(\"^[0-9]+.csv\", filename) and int(filename[:filename.find('.')]) in ids:\n",
    "            list_of_datasets.append(load_file(path_data+filename))\n",
    "            list_of_ids.append(int(filename[:filename.find('.')]))\n",
    "    return list_of_datasets, list_of_ids\n",
    "\n",
    "# Load files\n",
    "\n",
    "def load_file(path):\n",
    "    return pd.read_csv(path, sep=\";\", header=0, infer_datetime_format=True, parse_dates=['timestamp'], index_col=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acknowledged-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = {'time_step': 15,\n",
    "             'is_house' : False,\n",
    "             'min_bat_id': None,\n",
    "             'max_bat_id': None,\n",
    "             'in_bat_id': []}\n",
    "ids = get_selected_ids(selection)\n",
    "list_of_datasets, list_of_ids = get_list_of_datasets(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale each building data and return an array with all the scaled datasets\n",
    "\n",
    "def scaling(data):\n",
    "    data_scaled = []\n",
    "    for i in range(len(data)):\n",
    "        df_bat = data[i]\n",
    "        scaler = MinMaxScaler()\n",
    "        bat_scaled = scaler.fit_transform(df_bat)\n",
    "        # if the datasets feature the temperature, it is scaled too and included in the model\n",
    "        if 'temperature' in df_bat.columns:\n",
    "            df_bat_scaled = pd.DataFrame({'timestamp':df_bat.index,\n",
    "                                          'active_power':bat_scaled[:,0],\n",
    "                                          'temperature':bat_scaled[:,1],\n",
    "                                          'bat_id':list_of_ids[i]}).set_index('timestamp')\n",
    "        else:\n",
    "            df_bat_scaled = pd.DataFrame({'timestamp':df_bat.index,\n",
    "                                          'active_power':bat_scaled[:,0],\n",
    "                                          'bat_id':list_of_ids[i]}).set_index('timestamp')\n",
    "        data_scaled.append(df_bat_scaled)\n",
    "\n",
    "# create sequences of a predetermined duration from time series\n",
    "# the model learns to reconstruct these sequences\n",
    "\n",
    "def create_sequences(sequence, time_steps):\n",
    "    output = []\n",
    "    for i in range(len(sequence) - time_steps):\n",
    "        if np.all(sequence.values[i : (i + time_steps)][:,1] == sequence.values[i : (i + time_steps)][0,1]):\n",
    "            output.append(sequence[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "# build train set and test set from the data obtained using get_list_of_datasets\n",
    "\n",
    "def get_train_test_sets(data, time_steps):\n",
    "    # scale data\n",
    "    data_scaled = scaling(data)\n",
    "    # if the datasets feature the temperature, it is included in the model \n",
    "    if 'temperature' in data[0].columns:\n",
    "        df_train = pd.DataFrame(columns=['timestamp','active_power','temperature','bat_id']).set_index('timestamp')\n",
    "        df_test = pd.DataFrame(columns=['timestamp','active_power','temperature','bat_id']).set_index('timestamp')\n",
    "    else:\n",
    "        df_train = pd.DataFrame(columns=['timestamp','active_power','bat_id']).set_index('timestamp')\n",
    "        df_test = pd.DataFrame(columns=['timestamp','active_power','bat_id']).set_index('timestamp')\n",
    "    # we take the first 80% of the dataset for training data and the rest for test data\n",
    "    for i in range(math.floor(len(data)*0.8)):\n",
    "        df_train = pd.concat([df_train,data_scaled[i]])\n",
    "    for i in range(math.floor(len(data)*0.8), len(data)):\n",
    "        df_test = pd.concat([df_test,data_scaled[i]])\n",
    "    # we create the sequences from the train and test data\n",
    "    X_train = create_sequences(df_train,time_steps)\n",
    "    X_test = create_sequences(df_test,time_steps)\n",
    "    \n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    if 'temperature' in df_train.columns:\n",
    "        X_train = X_train[:,:,0:2]\n",
    "        X_test = X_test[:,:,0:2]\n",
    "    else:\n",
    "        X_train = X_train[:,:,0]\n",
    "        X_test = X_test[:,:,0]\n",
    "    # remove sequences with NaNs\n",
    "    X_train = X_train[~np.isnan(X_train).any(axis=(1,2))]\n",
    "    X_test = X_test[~np.isnan(X_test).any(axis=(1,2))]\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(X.shape[1],X.shape[2]))\n",
    "    L1 = LSTM(32, activation='relu',return_sequences=True,\n",
    "              kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    D1 = Dropout(rate=0.2)(L1)\n",
    "    L2 = LSTM(4, activation='relu', return_sequences=False)(D1)\n",
    "    L3 = RepeatVector(X.shape[1])(L2)\n",
    "    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(32, activation='relu', return_sequences=True)(L4)\n",
    "    D2 = Dropout(rate=0.2)(L5)\n",
    "    output = TimeDistributed(Dense(X.shape[2]))(D2)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = get_train_test_sets(list_of_datasets)\n",
    "model = autoencoder_model(X_train)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 5\n",
    "# maybe not the best idea not to use a predetermined validation set for time series\n",
    "history = model.fit(X_train,X_train,epochs=nb_epochs,validation_split=0.1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot evolution of loss and validation loss\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history['loss'], label='Training loss')\n",
    "plt.plot(history['val_loss'], label='Validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = model.predict(X_train)\n",
    "# get reconstruction error for each sample of the training set\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)[:,0]\n",
    "plt.figure()\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel('Train MAE loss')\n",
    "plt.ylabel('Number of samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred = model.predict(temp_test)\n",
    "# get reconstruction error for each sample of the test set\n",
    "test_mae_loss = np.mean(np.abs(X_test_pred-temp_test), axis=1)[:,0]\n",
    "plt.figure()\n",
    "plt.hist(test_mae_loss, bins=20)\n",
    "plt.xlabel('Test MAE loss')\n",
    "plt.ylabel('Number of samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary threshold\n",
    "threshold = 0.15\n",
    "anomalies = test_mae_loss > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of anomalous data in the test set\n",
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[data_idx - (TIME_STEPS) + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)\n",
    "        \n",
    "# get values of anomalous data\n",
    "df_anomalies = df_test.iloc[anomalous_data_indices]\n",
    "\n",
    "# get ids of buildings that have anomalous data\n",
    "list_id = np.unique(df_anomalies['bat_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get consumption data of buildings that have anomalies\n",
    "list_dfs = []\n",
    "for bat_id in list_id:\n",
    "    df_bat = df_test[df_test['bat_id'] == bat_id]\n",
    "    list_dfs.append(df_bat)\n",
    "\n",
    "# get anomalous data samples \n",
    "list_dfs_anomalies = []\n",
    "for bat_id in list_id:\n",
    "    df_bat = df_anomalies[df_anomalies['bat_id']==bat_id]\n",
    "    list_dfs_anomalies.append(df_bat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot anomalous data alongside the consumption timeline of buildings\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(14,10))\n",
    "fig.tight_layout()\n",
    "n = len(list_id)\n",
    "for i in range(n):\n",
    "    df_bat = list_dfs[i]\n",
    "    df_bat_anomalies = list_dfs_anomalies[i]\n",
    "    plt.subplot(5,3,i+1)\n",
    "    plt.plot(df_bat[['active_power']])\n",
    "    plt.plot(df_bat_anomalies[['active_power']],linestyle='',marker='.',color='red')\n",
    "plt.savefig(path_data+'145buildings_anomalies.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
